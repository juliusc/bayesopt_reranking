# runs few steps with the following command
# comet-train --load_from_checkpoint /home/oplatek/.cache/huggingface/hub/models--Unbabel--wmt22-cometkiwi-da/snapshots/b3a8aea5a5f
# c22db68a554b92b3d96eb6ea75cc9/checkpoints/model.ckpt --cfg da/configs/regression_model.yaml 
unified_metric:
  class_path: comet.models.UnifiedMetric
  init_args:
    use_first_layers: null
    # use_first_layers: -1
    remove_unused_layers_from_encoder: false
    train_data: 
      - da/data/train.csv
    validation_data: 
      - da/data/dev.csv
    # test_data:
    #   - da/data/test.csv
    activations: Tanh
    # pretrained_model: microsoft/infoxlm-large
    encoder_model: XLM-RoBERTa
    dropout: 0.1
    # batch_size: 16
    # learning_rate: 1.0e-06
    # encoder_learning_rate: 1.0e-07
    batch_size: 16
    encoder_learning_rate: 1.0e-05
    learning_rate: 1.5e-04
    nr_frozen_epochs: 0.3  # check max_epochs currently five
    layerwise_decay: 0.98 # 0.95 -> 0.98 becauese 0.95 ** 24 == 0.292 and 0.98 ** 24 == 0.616
    warmup_steps:  100
    final_activation: null
    hidden_sizes:
      - 3072
      - 1024
    input_segments:
      - mt
      - src
    keep_embeddings_frozen: true
    layer_norm: false
    layer_transformation: sparsemax
    loss: mse
    # loss_lambda: 0.65  # word loss - should not be used
    optimizer: AdamW
    
trainer: trainer.yaml
early_stopping: early_stopping.yaml
model_checkpoint: model_checkpoint.yaml
